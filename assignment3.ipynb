{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Assignment 03\n",
        "subtitle: Hyperparameter Tunning\n",
        "date: 10/27/2023\n",
        "date-modified: last-modified\n",
        "date-format: long\n",
        "format:\n",
        "  html:\n",
        "    theme:\n",
        "      - cosmo\n",
        "      - theme.scss\n",
        "    toc: true\n",
        "    embed-resources: true\n",
        "    number-sections: true\n",
        "author:\n",
        "  - name: Landon Carpenter\n",
        "    affiliations:\n",
        "      - id: gu\n",
        "        name: Georgetown University\n",
        "        city: Washington\n",
        "        state: DC\n",
        "  - name: Nuoya Wu (Nora)\n",
        "    affiliations:\n",
        "      - ref: gu\n",
        "---"
      ],
      "id": "74d66a37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset and display the dataframe (2 Points).\n"
      ],
      "id": "69342c6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "\n",
        "df_shopping = pd.read_csv(\"online_shoppers_intention.csv\")\n",
        "\n",
        "print(df_shopping.head())"
      ],
      "id": "62949b55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use `describe` to provide statistics on the pandas Dataframe (2 Points).\n"
      ],
      "id": "31d54f9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_shopping.describe()"
      ],
      "id": "b11f1cc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_shopping[\"Revenue\"].value_counts()"
      ],
      "id": "fac83ce9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the dataset into a Training set and a Test set. Justify your preferred split (3 Points).\n"
      ],
      "id": "db0490b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# \"Revenue\" is the target variable \n",
        "\n",
        "X = df_shopping.drop(\"Revenue\", axis =1)\n",
        "y = df_shopping[\"Revenue\"]\n",
        "\n",
        "# Split the dataset into 80% training and 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "id": "b285aa0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# categorical variables for one-hot encoding\n",
        "X_train = pd.get_dummies(X_train)\n",
        "X_test = pd.get_dummies(X_test)\n",
        "\n",
        "\n",
        "X_train.head()"
      ],
      "id": "8a6d9317",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure that train and test set have the same columns after encoding\n",
        "X_train, X_test = X_train.align(X_test, axis=1, fill_value=0)"
      ],
      "id": "503a7f8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification Routine (12 Points):\n",
        "\n",
        "Execute a classification routine using RandomForestClassifier(), BaggingClassifier(), and XGboostclassifier(). Independently output the accuracy box plot as discussed in class. Use any package you are comfortable with (seaborn, matplotlib).\n",
        "\n",
        "## RandomForestClassifier():\n"
      ],
      "id": "2e32d708"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "from seaborn.palettes import color_palette\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import  RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier"
      ],
      "id": "6a94ab10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "random.seed(1276)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "#k-fold cross validation\n",
        "k=10"
      ],
      "id": "c28451be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rfc = RandomForestClassifier(random_state=1276)\n",
        "rfc_acc = cross_val_score(rfc, X_train, y_train, cv=k, scoring='accuracy')"
      ],
      "id": "74cff27e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BaggingClassifier():\n"
      ],
      "id": "75b9a105"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bc = BaggingClassifier(random_state=1276)\n",
        "bc_acc = cross_val_score(bc, X_train, y_train, cv=k, scoring='accuracy')"
      ],
      "id": "4d8de0a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGboostclassifier():\n"
      ],
      "id": "affee201"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xgb = XGBClassifier(random_state=1276)\n",
        "xgb_acc = cross_val_score(xgb, X_train, y_train, cv=k, scoring='accuracy')"
      ],
      "id": "382a3408",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization\n"
      ],
      "id": "b48591a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_acc = [rfc_acc, bc_acc, xgb_acc]\n",
        "my_labels = ['Random Forest', 'Bagging', 'XGBoost']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.boxplot(data=my_acc, palette='Set3')\n",
        "ax.set_xticklabels(my_labels, rotation=60)\n",
        "ax.set_title('Model Accuracy Comparison')\n",
        "ax.set_ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "id": "0e5eca57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without any specification (the deflaut n_estimator = 10 and max_samples =1), Random Forest model performs better overall with the median accuracy score above 0.90 \n",
        "\n",
        "(From the previous trial) For the same number of base estimators (100), the result shows that XGBoost Classifier performs the best in terms of accuracy in predicting customers' shopping intent.\n",
        "\n",
        "# Classification with GridSearchCV (8 Points):\n",
        "\n",
        "Replicate the classification from Q2 using GridsearchCV().\n"
      ],
      "id": "c620e62d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = {'n_estimators':  [5, 10, 100],\n",
        "            'random_state': [1276]\n",
        "            }\n",
        "\n",
        "rfc_grid = GridSearchCV(rfc, params, cv=k, scoring='accuracy', n_jobs=-1)\n",
        "rfc_grid.fit(X_train, y_train)"
      ],
      "id": "e670a702",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#show the best hyperparameters for rfc\n",
        "print(f\"Best parameters for Random Forest: {rfc_grid.best_params_}\")"
      ],
      "id": "a306d96a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After evaluating the random forest classifier with 3 different parameter combinations, the best parameters for rnadom forests would be 300 base estimators are used in the ensemble (maximum available option). \n"
      ],
      "id": "49c7aff0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = {'n_estimators': [5, 10, 100],\n",
        "            'random_state': [1276]\n",
        "            }\n",
        "bc_grid = GridSearchCV(bc, params, cv=k, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "bc_grid.fit(X_train, y_train)"
      ],
      "id": "71d8e28a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Best parameters for Bagging: {bc_grid.best_params_}\")"
      ],
      "id": "9c69cae5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = {'n_estimators':  [5, 10, 100],\n",
        "          # 'max_depth': [3, 5, 7],\n",
        "            'random_state': [1276]\n",
        "            }\n",
        "xgb_grid = GridSearchCV(xgb, params, cv=k, scoring='accuracy', n_jobs=-1)\n",
        "xgb_grid.fit(X_train, y_train)"
      ],
      "id": "391c4c96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Best parameters for XGBoost: {xgb_grid.best_params_}\")"
      ],
      "id": "15ca4f95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rfc_results = rfc_grid.cv_results_['mean_test_score']\n",
        "bc_results = bc_grid.cv_results_['mean_test_score']\n",
        "xgb_results = xgb_grid.cv_results_['mean_test_score']\n",
        "\n",
        "all = [rfc_results, bc_results, xgb_results]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.boxplot(data=all, palette='Set3')\n",
        "ax.set_xticklabels(my_labels, rotation=60)\n",
        "ax.set_title('Model Accuracy Comparison - Grid Search')\n",
        "ax.set_ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "id": "9ea16d87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The XGBoost performs the best even though it only needs 10 base estimators (whereas for the two two were 100)\n",
        "\n",
        "(From previous trial) The box plots show that Random Forest has the widest range of accuracy socres among the three models, whereas the bagging model is the most consistent with the highest mean accuracy scores. \n",
        "\n",
        "# Classification with RandomSearchCV (8 Points):\n",
        "\n",
        "Replicate the classification from Q2 using RandomSearchCV().\n"
      ],
      "id": "69c0b656"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = {'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'random_state': [1276]\n",
        "            }\n",
        "\n",
        "rfc_random = RandomizedSearchCV(rfc, params, cv=k, scoring='accuracy', n_jobs=-1)\n",
        "rfc_random.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters for Random Forest: {rfc_random.best_params_}\")"
      ],
      "id": "42152f18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = {'n_estimators': [100, 200, 300],\n",
        "            'max_samples': [100, 200, 300],\n",
        "            'random_state': [1276]\n",
        "            }\n",
        "\n",
        "bc_random = RandomizedSearchCV(bc, params, cv=k, scoring='accuracy', n_jobs=-1)\n",
        "bc_random.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters for Bagging: {bc_random.best_params_}\")"
      ],
      "id": "e764157e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = {'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'random_state': [1276]\n",
        "            }\n",
        "\n",
        "xgb_random = RandomizedSearchCV(xgb, params, cv=k, scoring='accuracy', n_jobs=-1)\n",
        "xgb_random.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters for XGBoost: {xgb_random.best_params_}\")"
      ],
      "id": "304fedd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rfc_rscores = rfc_random.cv_results_['mean_test_score']\n",
        "bc_rscores = bc_random.cv_results_['mean_test_score']\n",
        "xgb_rscores = xgb_random.cv_results_['mean_test_score']\n",
        "\n",
        "all = [rfc_rscores, bc_rscores, xgb_rscores]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.boxplot(data=all, palette='Set3')\n",
        "ax.set_xticklabels(my_labels, rotation=60)\n",
        "ax.set_title('Model Accuracy Comparison - Random Search')\n",
        "ax.set_ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "id": "30c3d100",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison and Analysis (5 Points):\n",
        "\n",
        "Compare the results from Q2, Q3, and Q4. Describe the best hyperparameters for all three experiments.\n"
      ],
      "id": "a91b88ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Best parameters for Random Forest: {rfc_grid.best_params_}\")\n",
        "print(f\"Best parameters for Bagging: {bc_grid.best_params_}\")\n",
        "print(f\"Best parameters for XGBoost: {xgb_grid.best_params_}\")"
      ],
      "id": "6f8ba19b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One of the key takeaways for me is that hyperparameters tuning really requires a deep understanding of the data and the modeling efforts. Understanding the nature of the data and also the problem can be very helpful. For example, the shopping intent dataset has many features and it might make sense to adjsut \"max_features\" to find the right number. The \"max_depth\" can control overfitting and also ensure computational efficiency. Overall, we do see that most models performed around the 0.90 accuracy score threshold which is a very good sign. Hyperparameter tuning is both an art and a science. While there are systematic methods to search for the best parameters, intuition on understanding the data and selecting the right parameters for tuning play a significnat role as well. \n"
      ],
      "id": "699a6807"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}